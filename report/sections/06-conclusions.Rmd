

####**Conclusions**

While the final results may not have been riveting, it's still worth noting that the ridge and lasso regressions definitely outperformed the basic OLS function in predictive power. We find the ridge to have the slightest edge over the other models, but not to a particularly extreme degree. The standardized credit data set may just be pretty linear - none of the models deviated very strongly from the basic OLS method. It may not be a juicy result, but it's still a worthwhile conclusion to draw from the data - with some extensive analysis, we can see something that we weren't able to see at the outset, i.e., that the 11 predictors have a somewhat linear role in predicting Balance. This, on its own, could be seen as a pretty profound conclusion!

A couple final thoughts - is there a better way to choose the number of components in plsr and pcr? I find it hard to believe, but possible, that under both regression methods there was no increase in strength at all over just the basic OLS method. Is choosing more components sort of like how R^2 increases proportionately with the number of predictors you have? It's hard to say, and I'm not confident enough myself to state anything too assuredly here. Additionally - why did the ridge regression have those 2 deviations in coefficient values from OLS and Lasso? I don't know for certain, but that slight alteration actually served to increase the quality of the model, so perhaps the lambda tuning parameter piece of the ridge regression function performed the best, mathematically, over this data set. At least the ridge can predict a little better than OLS! We performed something of actual value! Maybe, even in the real world, some things are simply pretty darn linear...
